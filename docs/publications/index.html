<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Lilly Chin">
    <link rel="icon" href="/favicon.png" />
    <title>Publications | MERGe Lab</title>

    
    
    <link rel="stylesheet" href="//localhost:1313/style.min.59c17c3997448d5fefc72dc7533c719e55f79ae8be14989acf2c0390eaef56e8.css" />
    <link rel="stylesheet" href="//localhost:1313/fonts.min.f2be84683201ed3b3b7c8ca1ee0cd9ed6853907cc45eda7d24fc5829d3e440e8.css" />
    
<script src="//localhost:1313/rot13.min.ac65fd52f3ea0746e80f9c808c01237c4ba8739b18f44c10c61f2291b006e1e6.js"></script>


  </head>

  <body>
    <nav class="header">

    
     
    
    
    
    
    <a href="/">
      <img
        loading="lazy"
        sizes="(min-width: 35em) 3000px, 400px, 100vw (max-width: 3000)" 
        srcset=' /img/bannerLarge_hu3c9acbf620b945beb3f4ce2c44c55633_83983_100x0_resize_box_3.png 200w 
         /img/bannerLarge_hu3c9acbf620b945beb3f4ce2c44c55633_83983_400x0_resize_box_3.png 400w 
         /img/bannerLarge_hu3c9acbf620b945beb3f4ce2c44c55633_83983_600x0_resize_box_3.png 600w 
         /img/bannerLarge_hu3c9acbf620b945beb3f4ce2c44c55633_83983_800x0_resize_box_3.png800w  '
        
          src="/img/bannerLarge_hu3c9acbf620b945beb3f4ce2c44c55633_83983_600x0_resize_box_3.png"
        
        alt='MERGe Lab banner image'
        style="margin:  0em 0em;"
      />
    </a>

    <ul class="menu">
      
      <li>
          
            <a href="//localhost:1313/people/">People</a>
          
      </li>
      
      <li>
          
            <a href="//localhost:1313/research/">Research</a>
          
      </li>
      
      <li>
          
            <a href="//localhost:1313/publications/">Publications</a>
          
      </li>
      
      <li>
          
            <a href="//localhost:1313/news/">News</a>
          
      </li>
      
      <li>
          
            <a href="//localhost:1313/resources/">Resources</a>
          
      </li>
      
    </ul>
  </nav>
  <p></p>
<h1>Publications</h1>





<p>In all papers below, * means equal contribution to the manuscript. Members of the lab have been underlined. Notable papers have been <span class="highlight">highlighted</span>.</p>
<p>For the most up-to-date list of publications, please check Professor Chin&rsquo;s <a href="https://scholar.google.com/citations?hl=en&amp;user=yPvT_i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar profile</a>.</p>
<hr>








<table class=pubs>
  

  
    <colgroup>
       <col span="1" style="width: 30%;">
    </colgroup>
  

  
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="img"><img src = "img" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Embedded Air Channels Transform Soft Lattices into Sensorized Grippers</strong><br>Annan Zhang*, <u>Lillian Chin*</u>, Daniel L. Tong, Daniela Rus<br><em>ICRA 2024</em><br>Paper | Video <br><br> Summary of the paper <!--highlight --></td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/grocery2024.png"><img src = "/img_static/pubs/grocery2024.png" alt = "A robot arm grabs a can off of a conveyor belt to pack it in a conveyor belt. Insets demonstrate the two cameras&#39; point of view and the sensorized gripper." style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Online Packing of Groceries Through Soft Fingers with Integrated Visual-Tactile Sensing</strong><br>Valerie Chen*, <u>Lillian Chin*</u>, Jeana Choi*, Annan Zhang*, Daniela Rus<br><em>RoboSoft 2024</em> <br>Paper | Video <br><br> We combine external vision, soft tactile sensors and motor-based proprioception to create an autonomous grocery packing system. In real-time, our system can grasp an object off a conveyor belt, estimate its size and stiffness and pack a box. Combining multiple modes of sensor feedback reduces in <u>9x fewer damaging packs</u> than a sensor-less baseline and 4.5x fewer damaging packs than a vision-only baseline.</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="img"><img src = "img" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>In-Hand Manipulation with a Simple Belted Parallel-Jaw Gripper</strong><br>Gregory Xie, Rachel Holladay, <u>Lillian Chin</u>, Daniela Rus<br><em>IEEE RA-L</em>, 2023 <br>Paper | Video <br><br> Summary of the paper</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/zhang2023machine.png"><img src = "/img_static/pubs/zhang2023machine.png" alt = "Machine learning for soft robot proprioception should match the data distribution and sensor inputs" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Machine Learning Best Practices for Soft Robot Proprioception</strong> <br>Annan Zhang*, Tsun-Hsuan Wang*, Ryan L. Truby, <u>Lillian Chin</u>, Daniela Rus<br><em>IROS 2023</em><br> <a href="https://www.annanzhang.com/data/pdf/zhang2023machine.pdf">Paper</a><br><br>Based on experiments on two large soft robotics datasets, we derive best practices for training neural networks that map sensor signals to soft robot shape.</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="image"><img src = "image" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Modelling Handed Shearing Auxetics: Selective Piecewise Constant Strain Kinematics and Dynamic Simulation</strong><br>Maximillian St√∂lzle, <u>Lillian Chin</u>, Ryan L. Truby, Daniela Rus, Cosimo Della Santina<br><em>RoboSoft 2023</em>. <br>Paper <br><br> Summary of the paper</td>
      
    </tr>
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/auxbot2022.png"><img src = "/img_static/pubs/auxbot2022.png" alt = "A robot made out of metal squares and triangles in the closed and expanded states" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Flipper-Style Locomotion through Strong Expanding Modular Robots</strong><br><u>Lillian Chin</u>, Max Burns*, Gregory Xie*, Daniela Rus<br><em>IEEE RA-L</em>, 2022. Also presented at ICRA 2022. <br><a href="https://ieeexplore.ieee.org/abstract/document/9976216">Paper</a> | Video | ICRA Talk <br><br> <!-- highlight --> We create a force-focused version of our previous expanding robot spheres. By driving an aluminum version of the jitterbug with a leadscrew, we created an expanding robot that can expand 1.6x its original size and <u>exert an expansion force 76x its own weight</u>. When this robot was used as a module for a larger robot, the overall structure was <u>an order of magnitude faster than existing modular robots</u> and could <u>haul 1.5x its own weight, even with some modules stalling</u></td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/zhang2022vision.jpg"><img src = "/img_static/pubs/zhang2022vision.jpg" alt = "Cameras are placed at the top of the handed shearing auxetic fingers to view internal state changes" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Vision-Based Sensing for Electrically-Driven Soft Actuators</strong><br>Annan Zhang, Ryan L. Truby, <u>Lillian Chin</u>, Shuguang Li, Daniela Rus<br><em>IEEE RA-L</em>, 2022. Also presented at IROS 2022.<br> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9866780">Paper</a> <br><br>We use cameras to record the interior of compliant electric actuators and train a neural network that maps the visual feedback to the actuator&rsquo;s tip pose. Our method presents a robust approach for sensorizing hollow-bodied actuators and provides accurate predictions in the presence of external disturbances.</td>
      
    </tr>
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/truby2022fluidic.png"><img src = "/img_static/pubs/truby2022fluidic.png" alt = "See-through rendered view of a lattice, showing internal fluid chambers" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Fluidic Innervation Sensorizes Structures from a Single Build Material</strong><br>Ryan L. Truby*, <u>Lillian Chin*</u>, Annan Zhang, Daniela Rus<br><strong>Science Advances</strong>, 2022<br><a href="https://www.science.org/doi/pdf/10.1126/sciadv.abq4385">Paper</a> | <a href="https://news.mit.edu/2022/materials-sense-movements-0810">MIT News</a> <br><br> We develop a new sensorization technique: embed empty air-filled channels within a 3D printed structure. As the structure is moved or pressed, the channels will report a measurable change in pressure. Using this technique, we collected <u>the largest soft robotics movement dataset (18 hours)</u>. This allowed us to predict a 4 degree-of-freedom robotic platform&rsquo;s movement to <u>within 8% error, using only 12 sensors</u> <!-- highlight --></td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="image"><img src = "image" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Learning Policies by Learning Rules</strong><br>Brandon Araki, Jeana Choi, <u>Lillian Chin</u>, Xiao Li, Daniela Rus<br><em>IEEE RA-L</em>, 2021. <br><a href="https://ieeexplore.ieee.org/abstract/document/9667222">Paper</a> <br><br> Summary of the paper</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/jeopardy2021.png"><img src = "/img_static/pubs/jeopardy2021.png" alt = "A picture of the author (Lilly Chin) on Jeopardy but her head is replaced with a clam" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>How to Survive a Public Faming: Understanding &ldquo;The Spiciest Memelord&rdquo; via the Temporal Dynamics of Involuntary Celebrification</strong><br><u>Lillian Chin</u><br><em>First Monday</em>, 2021. <br><a href="https://journals.uic.edu/ojs/index.php/fm/article/view/11674">Paper</a> | CUNY Talk <br><br> The author conducts an autoethnography about her experiences on Jeopardy, an American TV show. By reflecting on the harassment she got, she connects her experience to other people who were &ldquo;publicly famed&rdquo; exposing a gap in the critical literature on celebrity. She introduces the technique of &ldquo;radical reciprocity&rdquo; as a way to combat the effects of unwanted fame, on both the temporalities of celebrity and microcelebrity.</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="image"><img src = "image" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Co-Learning of Task and Sensor Placement for Soft Robotics</strong><br>Andy Spielberg*, Alexander Amini*, <u>Lillian Chin</u>, Woijech Matusik, Daniela Rus<br><em>IEEE RA-L</em>, 2021. <strong>Nominated for Best Paper at Robosoft 2021</strong><br><a href="https://ieeexplore.ieee.org/abstract/document/9345345">Paper</a> <br><br> Summary of the paper</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/3dprintHSA2021.jpg"><img src = "/img_static/pubs/3dprintHSA2021.jpg" alt = "A bent handed shearing auxetic tube, with another tube inside of it" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>A Recipe for Electrically-Driven Soft Robots via 3D Printed Handed Shearing Auxetics</strong><br>Ryan L. Truby*, <u>Lillian Chin*</u>,  Daniela Rus<br><em>IEEE RA-L</em>, 2021. Also presented at Robosoft 2021<br><a href="https://ieeexplore.ieee.org/abstract/document/9326362">Paper</a> <br><br> We introduce a new way of fabricating handed shearing auxetic fingers. Rather than laser cutting them, we print them using a resin-based printer. This allows us to experiment with more materials and make more modifications to the underlying geometry without sacrificing resolution.</td>
      
    </tr>
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="image"><img src = "image" alt = "alt-text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Multiplexed Manipulation: Versatile Multimodal Grasping via a Hybrid Soft Gripper</strong><br><u>Lillian Chin</u>, Felipe Barscevicius, Jeffrey Lipton, Daniela Rus<br><em>ICRA 2020</em><br><a href="https://dspace.mit.edu/handle/1721.1/137301.2">Paper</a> <br><br> Rather than have separate systems for different grasping modes, we introduce multiplexed manipulation: a gripper that combines parallel jaw, suction and soft finger grasping into one system. By combining these different modes together, we are able to grasp 20% more objects than single grasping modes alone. We are also able to perform rudimentary in-hand manipulation with flat objects by suctioning them up and pinching them between the fingers.</td>
      
        
          
        
        <td><!-- highlight --></td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/auxbot2019.png"><img src = "/img_static/pubs/auxbot2019.png" alt = "A robot made up of two layers of spring steel in its closed and expanded states." style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Modular Volumetric Actuators Using Motorized Auxetics</strong><br>Jeffrey Lipton, <u>Lillian Chin</u>, Jacob Miske, Daniela Rus<br><em>IROS 2019</em><br><a href="https://ieeexplore.ieee.org/abstract/document/8968187">Paper</a> <br><br> We use the mathematical model of &ldquo;rotating polygons for auxetic materials&rdquo; to create robots that can expand at will by rotating one degree-of-freedom. These spring-steel robots can expand 1.2x their original volume in 0.25 seconds. We demonstrate how these robots can perform vertical tube crawling through peristaltic locomotion.</td>
      
    </tr>
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/recycling2019.png"><img src = "/img_static/pubs/recycling2019.png" alt = "A robot waits over a conveyor belt next to recycling bins" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Automated Recycling Separation Enabled by Soft Robotic Material Classification</strong><br><u>Lillian Chin</u>, Jeffrey Lipton, Michelle C Yuen, Rebecca Kramer-Bottiglio, Daniela Rus<br><strong>Won Best Poster at Robosoft 2019</strong><br><a href="https://dspace.mit.edu/handle/1721.1/124001">Paper</a> | Video | <a href="https://www.bbc.com/news/av/technology-47826476/the-robot-that-sorts-out-recycling">BBC</a> | <a href="https://www.scientificamerican.com/article/can-robots-help-pick-up-after-the-recycling-crisis/">Scientific American</a> | <a href="https://www.youtube.com/watch?v=1mxaN_xqQh4">CNBC</a> <br><br> Through collaboration with Michelle Yuen&rsquo;s (TEMP LINK) soft capactitve strain and pressure sensors, we extended our stiffness and size algorithm to now sort objects based on material: metal, paper and plastic. We demonstrated this on a mock recycling system, highlighting how the overall system was puncture resistant and effective on pathological examples.</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/icra2019.jpg"><img src = "/img_static/pubs/icra2019.jpg" alt = "A robot hand. One finger is covered with a gray sock while the other shows a handed shearing auxetic finger with a strain sensor on the back and a pressure sensor on the front." style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>A Simple Electric Soft Robotic Gripper with High-deformation Haptic Feedback</strong><br><u>Lillian Chin</u>, Michelle C Yuen, Jeffrey Lipton, Luis H Trueba, Rebecca Kramer-Bottiglio, Daniela Rus<br><em>ICRA 2019</em><br><a href="https://dspace.mit.edu/handle/1721.1/121154">Paper</a> | Video <br><br> Through collaboration with Michelle Yuen&rsquo;s (TEMP LINK) soft capactitve strain and pressure sensors, we created a simple hand that could tell the stiffness and size of objects. With linear regression and a test set of 4 objects, we could estimate radius to within 33% and tell hard from soft with 78% accuracy.</td>
      
    </tr>
  
    
    <tr class = "highlight">
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/hsa2018.png"><img src = "/img_static/pubs/hsa2018.png" alt = "A handed shearing auxetic extending as it is twisted" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Compliant Electric Actuators Based on Handed Shearing Auxetics</strong><br><u>Lillian Chin</u>, Jeffrey Lipton, Robert MacCurdy, John Romanishin, Chetan Sharma, Daniela Rus<br><em>RoboSoft 2018</em><br><a href="https://dspace.mit.edu/handle/1721.1/116908">Paper</a> | Video <br><br> By combining handed shearing auxetics of opposite handedness together, we can create a linear actuator. By adding a constraint to the pattern, we can create soft fingers. From these building blocks, we create a 4 degree-of-freedom robotic platform that can <u>extend up to 60% of its original length and twist up to 280 degrees</u>. We also create a soft robotic hand that is <u>20x more power efficient and 2x faster</u> than comparable pneuamtic-based soft hands, while achieving similar grasp performance. <!-- highlight --></td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/hsaMath2018.png"><img src = "/img_static/pubs/hsaMath2018.png" alt = "Render showing how a repeated unit cell of a parallelogram and a rectangle creates a handed shearing auxetic structure" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Handedness in Shearing Auxetics Creates Rigid and Compliant Structures</strong><br>Jeffrey Ian Lipton, Robert MacCurdy, Zachary Manchester, <u>Lillian Chin</u>, Daniel Cellucci, Daniela Rus<br><strong>Science</strong>, 2018<br><a href="https://www.science.org/doi/full/10.1126/science.aar4586">Paper</a> <br><br> Traditional auxetic materials have a unit cell that is symmetric. As the material expands, the unit cells rotate against each other until reaching a maximum point of expansion and collapsing down into the opposite symmetry. If we intentionally break this symmetry, we can create shear movement and make it impossible to collapse from one handedness to another. We call this handed shearing auxetics and demonstrate the potential for this class of material to create actuators.</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="/img_static/pubs/forceps2018.png"><img src = "/img_static/pubs/forceps2018.png" alt = "Obstetrical forceps with a modified handle, allowing for quick change and rotation" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Obstetrical Forceps With Passive Rotation and Sensor Feedback</strong><br>Judith M Beaudoin, <u>Lillian T Chin</u>, Hannah M Zlotnick, Thomas M Cervantes, Alexander H Slocum, Julian N Robinson, Sarah C Lassey<br><em>Design of Medical Devices 2018</em><br><a href="https://asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T11A004/272007">Paper</a> <br><br> Summary of the paper</td>
      
    </tr>
  
    
    <tr>
    
      
      
        
          
            
            
        
          
            <td> <a href="image"><img src = "image" alt = "alt text" style = "margin: 0em;"> </a></td>
            
        
          
        
        <td><strong>Conformal Robotic Stereolithography</strong><br> TEMP <br><em>3D Printing and Additive Manufacturing</em>, 2016<br>Paper<br><br>Summary of the paper</td>
      
    </tr>
  
</table>




	<nav class = "footer">
	  <hr/>
			<p class = "text-muted" style="float: left; text-align: left;"><small>Last updated: Feb 21, 2024</small></p>

	  		<p class = "text-muted" style="float: right; text-align: right;"><small><img loading="lazy" src="/icons8-mail-24.png" alt="Email Icon" style="vertical-align:middle"><span id="obfuscate">merge-labatutlistsdotutexasdotedu</span></small></p>
	</nav>
	
	<script>
		unobfuscate("obfuscate", "\<n uers='znvygb\:zretr-yno\@hgyvfgf.hgrknf.rqh'\>zretr-yno\@hgyvfgf.hgrknf.rqh\<\/n\>");
	</script>
  </body>
</html>