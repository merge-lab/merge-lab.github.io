<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Lilly Chin"><link rel=icon href=/favicon.png><title>Publications | MERGe Lab</title>
<link rel=stylesheet href=/style.min.9edd1c8a1f5293e52e1b95d4a9d7f512548f20665ae85b2031579409c42dde60.css><link rel=stylesheet href=/fonts.min.f2be84683201ed3b3b7c8ca1ee0cd9ed6853907cc45eda7d24fc5829d3e440e8.css><script src=/rot13.min.ac65fd52f3ea0746e80f9c808c01237c4ba8739b18f44c10c61f2291b006e1e6.js></script></head><body><nav class=header><a href=/><img loading=lazy sizes="(min-width: 35em) 3000px, 400px, 100vw (max-width: 3000)" srcset='/img/bannerLarge_hu11513478325244007372.png 200w
/img/bannerLarge_hu4380684498514256737.png 400w
/img/bannerLarge_hu17129784453839154246.png 600w
/img/bannerLarge_hu4354791056891830574.png800w' src=/img/bannerLarge_hu17129784453839154246.png alt='MERGe Lab banner image' style=margin:0></a><ul class=menu><li><a href=/people/>People</a></li><li><a href=/research/>Research</a></li><li><a href=/publications/>Publications</a></li><li><a href=/news/>News</a></li><li><div class=dropdown><a href=/resources/>Resources</a><div class=dropdown-content><ul class=sub-menu><li><a href=/resources/lab>Lab</a></li><li><a href=/resources/career>Career</a></li></ul></div></div></li></ul></nav><p></p><h1>Publications</h1><p>In all papers below, * means equal contribution to the manuscript. Members of the lab have been underlined. Notable papers have been <span class=highlight>highlighted</span>.</p><p>For the most up-to-date list of publications, please check Professor Chin&rsquo;s <a href="https://scholar.google.com/citations?hl=en&amp;user=yPvT_i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar profile</a> and <a href=https://www.youtube.com/@LillianTChin>Youtube</a>. If you would like a copy of any paper, please <a href=https://litchin.wordpress.com/contact/>contact</a> Professor Chin.</p><hr><table class=pubs><col span=1 style=width:30%><tr class=highlight><td><a href=/img_static/pubs/icra2024.png><img src=/img_static/pubs/icra2024.png alt="A sensorized robot gripper grasps a mustard bottle. It is sensorized through embedded air channels within the cubic lattice fingers." style=margin:0></a></td><td><strong>Embedded Air Channels Transform Soft Lattices into Sensorized Grippers</strong><br>Annan Zhang*, <u>Lillian Chin*</u>, Daniel L. Tong, Daniela Rus<br><em>ICRA 2024</em><br><a href=https://www.annanzhang.com/data/pdf/zhang2024embedded.pdf>Paper</a><br><br>We create a sensorized parallel jaw gripper by printing cubic lattice fingers with embedded air channels. Through 3D printing, we have significantly more control over the location and positioning of the channel sensors. After characterizing the sensor performance from compression, bending and time-dependent effects, we use the gripper to estimate the weight and location of grasped objects. We are able to imitate vision-based tactile sensor results with only 12 sensors.</td></tr><tr><td><a href=/img_static/pubs/grocery2024.png><img src=/img_static/pubs/grocery2024.png alt="A robot arm grabs a can off of a conveyor belt to pack it in a conveyor belt. Insets demonstrate the two cameras' point of view and the sensorized gripper." style=margin:0></a></td><td><strong>Real-Time Grocery Packing by Integrating Vision, Tactile Sensing, and Soft Fingers</strong><br>Valerie Chen*, <u>Lillian Chin*</u>, Jeana Choi*, Annan Zhang*, Daniela Rus<br><em>RoboSoft 2024</em><br><a href=https://ieeexplore.ieee.org/document/10521917>Paper</a> | <a href="https://www.youtube.com/watch?v=qetYLCcejTw">Video</a> | <a href=https://techcrunch.com/2024/06/30/mits-soft-robotic-system-is-designed-to-pack-groceries/>TechCrunch</a> | <a href=https://www.popsci.com/technology/grocery-bagging-robot-self-checkout/>Popular Science</a><br><br>We combine external vision, soft tactile sensors and motor-based proprioception to create an autonomous grocery packing system. In real-time, our system can grasp an object off a conveyor belt, estimate its size and stiffness and pack a box. Combining multiple modes of sensor feedback reduces in <u>9x fewer damaging packs</u> than a sensor-less baseline and 4.5x fewer damaging packs than a vision-only baseline.</td></tr><tr><td><a href=/img_static/pubs/bop2024.png><img src=/img_static/pubs/bop2024.png alt="A parallel-jaw gripper uses belts embedded within a finger to rotate and translate a toy peach." style=margin:0></a></td><td><strong>In-Hand Manipulation with a Simple Belted Parallel-Jaw Gripper</strong><br>Gregory Xie, Rachel Holladay, <u>Lillian Chin</u>, Daniela Rus<br><em>IEEE RA-L</em>, 2023<br><a href=https://ieeexplore.ieee.org/abstract/document/10373080/>Paper</a><br><br>We create BOP (Belt Orienting Phalanges), a parallel-jaw gripper where each finger has two sets of belts embedded within. By controlling each of the belts, we can control a grasped object&rsquo;s roll, pitch and translation. We demonstrate several instances of in-hand manipulation, including fingernail-style lifting of a thing object and screwing in a lightbulb.</td></tr><tr><td><a href=/img_static/pubs/zhang2023machine.png><img src=/img_static/pubs/zhang2023machine.png alt="Machine learning for soft robot proprioception should match the data distribution and sensor inputs" style=margin:0></a></td><td><strong>Machine Learning Best Practices for Soft Robot Proprioception</strong><br>Annan Zhang*, Tsun-Hsuan Wang*, Ryan L. Truby, <u>Lillian Chin</u>, Daniela Rus<br><em>IROS 2023</em><br><a href=https://ieeexplore.ieee.org/document/10342379>Paper</a><br><br>Based on experiments on two large soft robotics datasets, we derive best practices for training neural networks that map sensor signals to soft robot shape. Specifically, we analyze a handed shearing auxetic based robot platform with internal pressure-based sensors and a pneumatic trunk with external piezoresistive strain sensors. This analysis suggest several best practices for training a neural net on soft robotics: using relative inputs, including the actuator position, and using gated recurrent units or long short-term memory networks.</td></tr><tr><td><a href=/img_static/pubs/cosimo2023.png><img src=/img_static/pubs/cosimo2023.png alt="Comparison of a twisted HSA robot in real-life, simulation and kinematic abstraction." style=margin:0></a></td><td><strong>Modelling Handed Shearing Auxetics: Selective Piecewise Constant Strain Kinematics and Dynamic Simulation</strong><br>Maximillian St√∂lzle, <u>Lillian Chin</u>, Ryan L. Truby, Daniela Rus, Cosimo Della Santina<br><em>RoboSoft 2023</em>.<br><a href=https://ieeexplore.ieee.org/abstract/document/10121989>Paper</a><br><br>This paper extends discrete Cosserat rod models to model the movement of handed shearing auxetics (HSAs). By translating the auxetic trajectories to a Cosserat rod + piecewise constant strain model, we can model a four degree of freedom robotic HSA platform to an accuracy of 0.3 mm for position and 0.07 radians for orientation.</td></tr><tr class=highlight><td><a href=/img_static/pubs/auxbot2022.png><img src=/img_static/pubs/auxbot2022.png alt="A robot made out of metal squares and triangles in the closed and expanded states" style=margin:0></a></td><td><strong>Flipper-Style Locomotion through Strong Expanding Modular Robots</strong><br><u>Lillian Chin</u>, Max Burns*, Gregory Xie*, Daniela Rus<br><em>IEEE RA-L</em>, 2022. Also presented at ICRA 2022.<br><a href=https://ieeexplore.ieee.org/abstract/document/9976216>Paper</a> | <a href="https://www.youtube.com/watch?v=8mhclLQWb_Q">ICRA Talk</a> | <a href="https://www.youtube.com/watch?v=l36GjQZS7vU&amp;list=PLdPBfRQbWBGXjGHT0ZJDUz-xapEMAcqFd&amp;index=2&amp;pp=iAQB">Mashable</a><br><br>We create a force-focused version of our previous expanding robot spheres. By driving an aluminum version of the jitterbug with a leadscrew, we created an expanding robot that can expand 1.6x its original size and <u>exert an expansion force 76x its own weight</u>. When this robot was used as a module for a larger robot, the overall structure was <u>an order of magnitude faster than existing modular robots</u> and could <u>haul 1.5x its own weight, even with some modules stalling</u></td></tr><tr><td><a href=/img_static/pubs/zhang2022vision.jpg><img src=/img_static/pubs/zhang2022vision.jpg alt="Cameras are placed at the top of the handed shearing auxetic fingers to view internal state changes" style=margin:0></a></td><td><strong>Vision-Based Sensing for Electrically-Driven Soft Actuators</strong><br>Annan Zhang, Ryan L. Truby, <u>Lillian Chin</u>, Shuguang Li, Daniela Rus<br><em>IEEE RA-L</em>, 2022. Also presented at IROS 2022.<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9866780">Paper</a><br><br>We use cameras to record the interior of compliant electric actuators and train a neural network that maps the visual feedback to the actuator&rsquo;s tip pose. Our method presents a robust approach for sensorizing hollow-bodied actuators and provides accurate predictions in the presence of external disturbances.</td></tr><tr class=highlight><td><a href=/img_static/pubs/truby2022fluidic.png><img src=/img_static/pubs/truby2022fluidic.png alt="See-through rendered view of a lattice, showing internal fluid chambers" style=margin:0></a></td><td><strong>Fluidic Innervation Sensorizes Structures from a Single Build Material</strong><br>Ryan L. Truby*, <u>Lillian Chin*</u>, Annan Zhang, Daniela Rus<br><strong>Science Advances</strong>, 2022<br><a href=https://www.science.org/doi/pdf/10.1126/sciadv.abq4385>Paper</a> | <a href=https://news.mit.edu/2022/materials-sense-movements-0810>MIT News</a><br><br>We develop a new sensorization technique: embed empty air-filled channels within a 3D printed structure. As the structure is moved or pressed, the channels will report a measurable change in pressure. Using this technique, we collected <u>the largest soft robotics movement dataset (18 hours)</u>. This allowed us to predict a 4 degree-of-freedom robotic platform&rsquo;s movement to <u>within 8% error, using only 12 sensors</u></td></tr><tr><td><a href=/img_static/pubs/brandon2022.png><img src=/img_static/pubs/brandon2022.png alt="Pseudocode for the HILO algorithm, that takes in a markov decision process and trajectories to return an optimal plan" style=margin:0></a></td><td><strong>Learning Policies by Learning Rules</strong><br>Brandon Araki, Jeana Choi, <u>Lillian Chin</u>, Xiao Li, Daniela Rus<br><em>IEEE RA-L</em>, 2021.<br><a href=https://ieeexplore.ieee.org/abstract/document/9667222>Paper</a><br><br>We present Hierarchical Inference with Logical Options (HILO), a Bayesian model that operates on linear temporal logic (LTL) formulas. By operating over LTL formulas and low-level trajectories, the model is able to find an optimal policy while maintaining interpretability. HILO outperforms other methods on planning tasks and is validated in the real world on a grocery packing task.</td></tr><tr><td><a href=/img_static/pubs/jeopardy2021.png><img src=/img_static/pubs/jeopardy2021.png alt="A picture of the author (Lilly Chin) on Jeopardy but her head is replaced with a clam" style=margin:0></a></td><td><strong>How to Survive a Public Faming: Understanding &ldquo;The Spiciest Memelord&rdquo; via the Temporal Dynamics of Involuntary Celebrification</strong><br><u>Lillian Chin</u><br><em>First Monday</em>, 2021.<br><a href=https://journals.uic.edu/ojs/index.php/fm/article/view/11674>Paper</a> | <a href="https://www.youtube.com/watch?v=2cLUMUEMUzc">CUNY, Queens College Talk</a><br><br>The author conducts an autoethnography about her experiences on Jeopardy, an American TV show. By reflecting on the harassment she got, she connects her experience to other people who were &ldquo;publicly famed&rdquo; exposing a gap in the critical literature on celebrity. She introduces the technique of &ldquo;radical reciprocity&rdquo; as a way to combat the effects of unwanted fame, on both the temporalities of celebrity and microcelebrity.</td></tr><tr><td><a href=/img_static/pubs/colearning2021.png><img src=/img_static/pubs/colearning2021.png alt="Overview of a machine learning algorithm to determine optimal sensor placement within a simulated gripper" style=margin:0></a></td><td><strong>Co-Learning of Task and Sensor Placement for Soft Robotics</strong><br>Andy Spielberg*, Alexander Amini*, <u>Lillian Chin</u>, Woijech Matusik, Daniela Rus<br><em>IEEE RA-L</em>, 2021. <strong>Nominated for Best Paper at Robosoft 2021</strong><br><a href=https://ieeexplore.ieee.org/abstract/document/9345345>Paper</a> | <a href="https://www.youtube.com/watch?v=MSa7D0FvxqY">Video</a> | <a href=https://news.mit.edu/2021/sensor-soft-robots-placement-0322>MIT News</a><br><br>We introduce a machine learning technique to design a soft robot&rsquo;s sensor placement for a specific task. We assume that the soft robot is simulated using the material point method and then sparsify the number of sensors in the overall structure while maintaining high performance. We demonstrate the viability of this task in simulation for grasp prediction, learned proprioception and control of a quadruped. Our method significantly outperforms human and machine learning baselines and is resistant to sensor noise.</td></tr><tr><td><a href=/img_static/pubs/3dprintHSA2021.jpg><img src=/img_static/pubs/3dprintHSA2021.jpg alt="A bent handed shearing auxetic tube, with another tube inside of it" style=margin:0></a></td><td><strong>A Recipe for Electrically-Driven Soft Robots via 3D Printed Handed Shearing Auxetics</strong><br>Ryan L. Truby*, <u>Lillian Chin*</u>, Daniela Rus<br><em>IEEE RA-L</em>, 2021. Also presented at Robosoft 2021<br><a href=https://ieeexplore.ieee.org/abstract/document/9326362>Paper</a><br><br>We introduce a new way of fabricating handed shearing auxetic fingers. Rather than laser cutting them, we print them using a resin-based printer. This allows us to experiment with more materials and make more modifications to the underlying geometry without sacrificing resolution.</td></tr><tr class=highlight><td><a href=/img_static/pubs/multiplexedManip2020.png><img src=/img_static/pubs/multiplexedManip2020.png alt="A robot gripper grabs a plastic banana in three ways: suction, parallel-jaw and soft fingers." style=margin:0></a></td><td><strong>Multiplexed Manipulation: Versatile Multimodal Grasping via a Hybrid Soft Gripper</strong><br><u>Lillian Chin</u>, Felipe Barscevicius, Jeffrey Lipton, Daniela Rus<br><em>ICRA 2020</em><br><a href=https://dspace.mit.edu/handle/1721.1/137301.2>Paper</a> | <a href="https://www.youtube.com/watch?v=NWzDpKNGdXk">Video</a> | <a href="https://www.youtube.com/watch?v=npoUdod4fvI">ICRA Talk</a><br><br>Rather than have separate systems for different grasping modes, we introduce multiplexed manipulation: a gripper that combines parallel jaw, suction and soft finger grasping into one system. By combining these different modes together, we are able to grasp 20% more objects than single grasping modes alone. We are also able to perform rudimentary in-hand manipulation with flat objects by suctioning them up and pinching them between the fingers.</td><td></td></tr><tr><td><a href=/img_static/pubs/auxbot2019.png><img src=/img_static/pubs/auxbot2019.png alt="A robot made up of two layers of spring steel in its closed and expanded states." style=margin:0></a></td><td><strong>Modular Volumetric Actuators Using Motorized Auxetics</strong><br>Jeffrey Lipton, <u>Lillian Chin</u>, Jacob Miske, Daniela Rus<br><em>IROS 2019</em><br><a href=https://ieeexplore.ieee.org/abstract/document/8968187>Paper</a><br><br>We use the mathematical model of &ldquo;rotating polygons for auxetic materials&rdquo; to create robots that can expand at will by rotating one degree-of-freedom. These spring-steel robots can expand 1.2x their original volume in 0.25 seconds. We demonstrate how these robots can perform vertical tube crawling through peristaltic locomotion.</td></tr><tr><td><a href=/img_static/pubs/recycling2019.png><img src=/img_static/pubs/recycling2019.png alt="A robot waits over a conveyor belt next to recycling bins" style=margin:0></a></td><td><strong>Automated Recycling Separation Enabled by Soft Robotic Material Classification</strong><br><u>Lillian Chin</u>, Jeffrey Lipton, Michelle C Yuen, Rebecca Kramer-Bottiglio, Daniela Rus<br><strong>Won Best Poster at Robosoft 2019</strong><br><a href=https://dspace.mit.edu/handle/1721.1/124001>Paper</a> | <a href="https://www.youtube.com/watch?v=slAZS78QmoA">Video</a> | <a href=https://www.bbc.com/news/av/technology-47826476/the-robot-that-sorts-out-recycling>BBC</a> | <a href=https://www.scientificamerican.com/article/can-robots-help-pick-up-after-the-recycling-crisis/>Scientific American</a> | <a href="https://www.youtube.com/watch?v=1mxaN_xqQh4">CNBC</a><br><br>Through collaboration with <a href=https://www.michellecyuen.com>Michelle Yuen</a>&rsquo;s soft capactitve strain and pressure sensors, we extended our stiffness and size algorithm to now sort objects based on material: metal, paper and plastic. We demonstrated this on a mock recycling system, demonstrating how the overall system was puncture resistant and effective on pathological examples.</td></tr><tr><td><a href=/img_static/pubs/icra2019.jpg><img src=/img_static/pubs/icra2019.jpg alt="A robot hand. One finger is covered with a gray sock while the other shows a handed shearing auxetic finger with a strain sensor on the back and a pressure sensor on the front." style=margin:0></a></td><td><strong>A Simple Electric Soft Robotic Gripper with High-deformation Haptic Feedback</strong><br><u>Lillian Chin</u>, Michelle C Yuen, Jeffrey Lipton, Luis H Trueba, Rebecca Kramer-Bottiglio, Daniela Rus<br><em>ICRA 2019</em><br><a href=https://dspace.mit.edu/handle/1721.1/121154>Paper</a> | <a href="https://www.youtube.com/watch?v=VASZ8P1mPsw">Video</a><br><br>Through collaboration with <a href=https://www.michellecyuen.com>Michelle Yuen</a>&rsquo;s soft capactitve strain and pressure sensors, we created a simple hand that could tell the stiffness and size of objects. With linear regression and a test set of 4 objects, we could estimate radius to within 33% and tell hard from soft with 78% accuracy.</td></tr><tr class=highlight><td><a href=/img_static/pubs/hsa2018.png><img src=/img_static/pubs/hsa2018.png alt="A handed shearing auxetic extending as it is twisted" style=margin:0></a></td><td><strong>Compliant Electric Actuators Based on Handed Shearing Auxetics</strong><br><u>Lillian Chin</u>, Jeffrey Lipton, Robert MacCurdy, John Romanishin, Chetan Sharma, Daniela Rus<br><em>RoboSoft 2018</em><br><a href=https://dspace.mit.edu/handle/1721.1/116908>Paper</a> | <a href="https://www.youtube.com/watch?v=53NXnPTG9Ik">Video</a><br><br>By combining handed shearing auxetics of opposite handedness together, we can create a linear actuator. By adding a constraint to the pattern, we can create soft fingers. From these building blocks, we create a 4 degree-of-freedom robotic platform that can <u>extend up to 60% of its original length and twist up to 280 degrees</u>. We also create a soft robotic hand that is <u>20x more power efficient and 2x faster</u> than comparable pneuamtic-based soft hands, while achieving similar grasp performance.</td></tr><tr><td><a href=/img_static/pubs/hsaMath2018.png><img src=/img_static/pubs/hsaMath2018.png alt="Render showing how a repeated unit cell of a parallelogram and a rectangle creates a handed shearing auxetic structure" style=margin:0></a></td><td><strong>Handedness in Shearing Auxetics Creates Rigid and Compliant Structures</strong><br>Jeffrey Ian Lipton, Robert MacCurdy, Zachary Manchester, <u>Lillian Chin</u>, Daniel Cellucci, Daniela Rus<br><strong>Science</strong>, 2018<br><a href=https://www.science.org/doi/full/10.1126/science.aar4586>Paper</a><br><br>Traditional auxetic materials have a unit cell that is symmetric. As the material expands, the unit cells rotate against each other until reaching a maximum point of expansion and collapsing down into the opposite symmetry. If we intentionally break this symmetry, we can create shear movement and make it impossible to collapse from one handedness to another. We call this handed shearing auxetics and demonstrate the potential for this class of material to create actuators.</td></tr><tr><td><a href=/img_static/pubs/forceps2018.png><img src=/img_static/pubs/forceps2018.png alt="Obstetrical forceps with a modified handle, allowing for quick change and rotation" style=margin:0></a></td><td><strong>Obstetrical Forceps With Passive Rotation and Sensor Feedback</strong><br>Judith M Beaudoin, <u>Lillian T Chin</u>, Hannah M Zlotnick, Thomas M Cervantes, Alexander H Slocum, Julian N Robinson, Sarah C Lassey<br><em>Design of Medical Devices 2018</em><br><a href=https://asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T11A004/272007>Paper</a><br><br>We introduce a new design for obstetric forceps based on clinician feedback. To allow for delivery of babies in non-standard (non-occiput anterior) positions, we create forceps with a passively rotating handle and quick change blades for easier use. We also explore an additional sensing attachment to provide force / direction feedback to obstetricians to reduce the learning curve for forceps use.</td></tr><tr><td><a href=/img_static/pubs/agstev2016.png><img src=/img_static/pubs/agstev2016.png alt="A robot arm with a projector attached to it points at a bone on a turntable." style=margin:0></a></td><td><strong>Conformal Robotic Stereolithography</strong><br>Adam G. Stevens, C. Ryan Oliver, Matthieu Kirchmeyer, Jieyuan Wu, <u>Lillian Chin</u>, Erik S. Polsen, Chad Archer, Casey Boyle, Jenna Garber, and A. John Hart<br><em>3D Printing and Additive Manufacturing</em>, 2016<br><a href=https://www.liebertpub.com/doi/abs/10.1089/3dp.2016.0042>Paper</a><br><br>By attaching a projector to the end of a six-axis robot arm, we are able to perform stereolithography printing on curved surfaces. Rather than be limited to the standard planar layer of traditional 3D printing, we can create freeform high precision prints.</td></tr></table><nav class=footer><hr><p class=text-muted style=float:left;text-align:left><small>Last updated: Sep 3, 2024</small></p><p class=text-muted style=float:right;text-align:right><small><img loading=lazy src=/icons8-mail-24.png alt="Email Icon" style=vertical-align:middle><span id=obfuscate>merge-labatutlistsdotutexasdotedu</span></small></p></nav><script>unobfuscate("obfuscate","<n uers='znvygb:zretr-yno@hgyvfgf.hgrknf.rqh'>zretr-yno@hgyvfgf.hgrknf.rqh</n>")</script></body></html>